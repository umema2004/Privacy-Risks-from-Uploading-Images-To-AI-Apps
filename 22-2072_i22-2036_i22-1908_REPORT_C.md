![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.001.png)

**Assignment No.1** 

Information Security ![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.002.png)

**Privacy Risks from Uploading Images To AI Apps**

***Submitted to:* Ma’am Azka Atiq** 

***Submitted by:*** \
Umema Ashar 22I-2036 Emad Iqbal 22I-2072 Hamza Asad 22I-1908 

***Date of Submission:***  18th September 2025 

***Section:***  DS-C 

Report: Introduction, Literature Review & Critical Analysis, and Results Summary 

1. Introduction to the Base Paper 

   The *RobustFace* framework proposes a **pre-processing defense mechanism** for face recognition systems, designed to restore adversarially perturbed face images before recognition or detection occurs. The authors argue that adversarial perturbations, whether pixel-based, generative, or geometric, displace facial data from the natural image manifold; consequently, restoring such images toward this manifold enhances recognition robustness. 

   Three distinct categories of attacks are examined: (1) a gradient-based perturbation (P-FGSM variant) representing pixel-level distortions; (2) StyleGAN-based manipulations that simulate generative adversarial perturbations; and (3) Fast Landmark Manipulation (FLM), which alters facial geometry. Together, these attacks provide a comprehensive evaluation of the defense method against a broad spectrum of adversarial threats. 

   The proposed restoration pipeline employs a **Bilateral Filter (BL)** for denoising, preserving edges while suppressing adversarial noise, followed by an **Enhanced Deep Super-Resolution (EDSR)** network to refine structural and textural fidelity. After restoration, **Weighted Local Magnitude Pattern (WLMP)** features are extracted to capture local intensity variations, and these features are classified using conventional machine learning models such as Support Vector Machines (SVM). This multi-stage pipeline aims to recover recognition performance that has been degraded by adversarial interference. 

   Empirical evaluations conducted on two real-world facial datasets, **CelebA** and **LFW (Labeled Faces in the Wild)**, demonstrate substantial improvements in recognition accuracy across all tested attack types. The study shows that image-restoration-based preprocessing, when combined with robust feature encoding, can significantly mitigate the effects of adversarial perturbations on face recognition systems. 

   Beyond its defensive implications, *RobustFace* also highlights an important privacy concern. The same adversarial and generative manipulations explored in research contexts can occur in public AI platforms, where user-uploaded facial images are vulnerable to intentional or unintentional alterations. Therefore, the paper holds dual relevance as both a technical contribution to adversarial defense and a reflection on emerging privacy risks in AI-based image generation. 

2. Literature Review (LR): Situating RobustFace 
1. Research strands relevant to your topic 
1. **Adversarial attacks on face systems** 
- A substantial body of research focuses on crafting perturbations that deceive face recognition models. Methods such as FGSM, PGD, and the Carlini–Wagner attack demonstrate that imperceptible pixel-level changes can lead to severe misidentification or misverification. 
2. **Geometric / landmark attacks** 
- Several studies explore manipulations of facial landmarks or geometry, such as the Fast Landmark Manipulation (FLM) technique, which alters structural attributes of the face to influence recognition outcomes. These approaches produce semantically coherent yet adversarially modified facial images. 
3. **GAN-based manipulations / deepfakes** 
- Generative Adversarial Networks (GANs), including StyleGAN, have enabled the creation of highly realistic synthetic facial images. These manipulations can modify facial features such as age, gender, and expression, or perform complete face swaps. Such capabilities raise significant privacy and security concerns when applied maliciously. 
4. **Preprocessing / restoration defenses** 
- A complementary research strand investigates input transformation defenses that aim to restore adversarially perturbed images to their clean manifold representations. Techniques include denoising, bilateral filtering, super-resolution, and other restoration approaches that mitigate perturbations before recognition. 
5. **Adversarial training and robust feature learning** 
- These studies enhance model robustness by incorporating adversarial examples into training or learning embeddings that remain invariant under perturbations. This approach typically modifies the recognition model itself rather than relying on preprocessing. 
6. **Privacy-centered works on image uploads** 
- Recent work examines the privacy implications of uploading facial data to online platforms. Studies address dataset leakage, embedding inversion, and membership inference attacks, alongside broader legal and regulatory concerns regarding image ownership and consent. 
2. Position of *RobustFace* within the Literature 

*RobustFace* integrates elements from multiple research domains. Its threat model draws from adversarial pixel attacks, geometric landmark attacks, and GAN-based manipulations, while its defense strategy aligns with preprocessing-based restoration approaches. The framework distinguishes itself by combining bilateral filtering and super-resolution (BL + SR) with handcrafted feature extraction through Weighted Local Magnitude Pattern (WLMP) encoding. In contrast to most existing defenses that rely on learned embeddings or adversarial training, *RobustFace* adopts a **model-agnostic** preprocessing perspective. 

3. Addressed Gaps in the Literature 

Existing defenses predominantly focus on pixel-level adversarial perturbations. Only a limited number of studies evaluate geometric and generative adversarial manipulations together. *RobustFace* contributes by conducting a unified evaluation across three distinct attack families: pixel-based (P-FGSM), generative (StyleGAN-based), and geometric (FLM) attacks. Furthermore, the paper emphasizes the practicality of a model-independent preprocessing defense, which can be applied on the client side or integrated with diverse downstream recognition systems. 

3. Critical Review of the Base Paper 
1. Strengths 
- **Broad attack coverage:** Inclusion of pixel (P-FGSM), generative (StyleGAN-based), and geometric (FLM) attacks increases external validity compared to single-attack evaluations. 
- **Intuitive defense approach:** Restoration to the natural manifold is a conceptually simple, model-agnostic defense that can be adopted as a preprocessing stage. 
- **Empirical gains:** Reported accuracy improvements (especially for StyleGAN attacks) indicate restoration can meaningfully help recognition. 
2. Weaknesses & concerns 
- **Limited methodological transparency:** Although the methodology outlines key components such as bilateral filtering, EDSR super-resolution, and WLMP extraction, the paper omits specific hyperparameters, dataset splits, and implementation details. The absence of released code reduces reproducibility. 
- **Insufficient Baseline Comparisons:** The study does not provide quantitative comparisons with established defense baselines such as adversarially trained models, denoising autoencoders, or randomized input defenses. Without these benchmarks, it is difficult to assess the relative performance of the proposed method. 
- **No evaluation against adaptive attacks:** The defense is not tested against adaptive adversaries that optimize perturbations while considering the defense mechanism. As a result, its robustness under white-box conditions remains unverified. 
- **Potential vulnerability to advanced generative attacks:** The restoration stage may not be effective against future GAN-based adversarial methods that explicitly exploit or circumvent restoration filters. 
- **Computational overhead unreported:** The paper does not discuss runtime or computational cost. High latency may limit deployment in real-time or client-side applications such as mobile authentication. 
- **Reliance on handcrafted features:** While WLMP descriptors enhance interpretability, modern face recognition systems predominantly use learned deep embeddings. This reliance on handcrafted features may restrict scalability or accuracy compared to contemporary embedding-based methods. 
- **Limited privacy framing:** The paper addresses technical robustness rather than broader privacy governance issues. It focuses on detecting and mitigating adversarial attacks rather than analyzing data collection, storage, or consent mechanisms. 
3. Threat model alignment for privacy work 

Although the primary focus of *RobustFace* is adversarial robustness, its findings have implications for privacy research. The work demonstrates that facial images can be manipulated through gradient-based, geometric, or generative methods, confirming the technical feasibility of image- based privacy threats such as identity spoofing and deepfake generation. However, the paper does not analyze how such images are stored or governed within AI systems, nor does it address risks related to model extraction, membership inference, or user consent. Consequently, *RobustFace* is best understood as a **technical contribution** to adversarial defense research that indirectly informs privacy risk assessments. 

4. Summary of Results Presented in the Base Paper 

   **Main experimental claims** 

   The *RobustFace* study demonstrates that incorporating a **bilateral filtering and super-resolution (BL + SR)** restoration step prior to feature extraction and classification enhances recognition accuracy under multiple adversarial conditions. The restoration process is shown to improve model performance against three categories of attacks: pixel-based perturbations (P-FGSM), generative adversarial manipulations (StyleGAN-based), and geometric alterations (Fast Landmark Manipulation, FLM). 

   **Representative numbers (before and after restoration)** 

- **P-FGSM:** 98.75% → 99.00% (small absolute improvement; indicates the model was already robust or the attack weak) 
- **StyleGAN-based manipulations:** 77.94% to 85.25% (substantial improvement; suggests restoration helps recover content altered by generative manipulations) 
- **FLM (landmark manipulation):** 65.52% to 69.50% (modest improvement; geometric attacks are partially mitigated) 

**Additional empirical notes** 

- Figures in the public pages show comparative accuracy plots and visual examples of restored images. Restoration appears to reduce visible artifacts and increase WLMP-based recognition scores. 
- The paper reports experiments on **two real-world face datasets**, giving some ecological validity. 

**Caveats about the results** 

- The magnitude of improvement varies widely by attack type, restoration works better for some attacks (StyleGAN) than others (FLM). This heterogeneity suggests restoration is not a universal fix. 
- No reported results (in the accessible summary) for: 
- **adaptive attacks** optimized against the entire pipeline, 
- **runtime / throughput** (ms/image), 
- **tradeoffs between visual quality and robustness** (PSNR / SSIM / LPIPS), 
- **comparisons to adversarial training** or other defenses. 
5. Implementation & Methodology  
1. Objective and Scope 

This implementation aims to reproduce and validate the key principles of the *RobustFace* paper (Sadu et al., 2024) using the **CelebA** dataset. 

Instead of replicating the exact restoration architecture (Blind Deblurring + Super-Resolution), a simplified but conceptually equivalent **bilateral restoration pipeline** was implemented. 

The focus was on verifying whether restoration preprocessing meaningfully reduces the impact of adversarial perturbations on downstream recognition or feature separability. 

2. Dataset Preparation 
- **Dataset:** CelebA face dataset with over 200 000 annotated face images. 
- **Splits:** Train = 162 770, Validation = 19 867, Test = 19 962 (from list\_eval\_partition.csv). 
- **Input:** Grayscale 128 × 128 pixels, normalized to [0, 1]. 
- **Task:** Binary classification of the “Smiling” attribute, used as a proxy for face-recognition performance. 
3. Model Architecture 

A lightweight convolutional model, **SmallFaceNet**, was trained from scratch. 

It consists of stacked Conv–BatchNorm–ReLU blocks with adaptive pooling and two fully connected layers. 

This was chosen for efficiency and interpretability rather than raw accuracy, mirroring the “model- agnostic defense” philosophy of *RobustFace*. 

**Optimizer:** Adam (learning rate = 1e-3)    **Batch size:** 64    **Dropout:** 0.3    **Device:** CUDA **Performance Summary** 

**Metric  Best Epoch  Validation Accuracy  Final Training Loss** SmallFaceNet  3  93.07 %  0.175 

![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.003.jpeg)

***Figure 5.1: Training vs Validation Accuracy and Loss Curve*** 

4. Adversarial Attack Generation 

Two adversarial threat models were simulated: 

1. **BIM (Basic Iterative Method)** — gradient-based pixel perturbations (P-FGSM equivalent). 
1. **FLM (Fast Landmark Manipulation)** — geometric landmark-based distortions. 

Both attacks were applied on the validation/test splits, producing around 20 000 adversarial samples each. 

Files were stored in /kaggle/working/adv\_test and /kaggle/working/adv\_test\_flm. 

5. Restoration Pipeline 

The restoration module acted as a lightweight approximation of the RobustFace BL + SR stage: 

- **Method:** Bilateral filter with (d = 5, σColor = 75, σSpace = 75) 
- **Purpose:** Reduce high-frequency noise and approximate manifold projection. 
- **Output:** Restored adversarial images saved to /kaggle/working/restored\_test. 

![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.004.jpeg)

***Figure 5.2: Example Original–Adversarial–Restored Triplets*** 

6. Feature Extraction and Classification 

For evaluation, the **Weighted Local Magnitude Pattern (WLMP)** descriptor was implemented to capture texture and gradient-magnitude variations. 

- WLMP histograms (256-D) were computed for each image. 
- Features from clean, adversarial, and restored images were used to train standard classifiers: **SVM (linear & RBF)**, **Random Forest**, and **k-NN**. 

This procedure measures **adversarial detectability** rather than recognition accuracy — consistent with testing whether restoration reduces the adversarial footprint. 

![ref1] ![ref2]

***Figure 5.3: WLMP Feature Distribution (before vs after restoration) – t-SNE*** 

![ref3] ![ref4]

***Figure 5.3: WLMP Feature Distribution (before vs after restoration) – PCA*** 

6\. Results and Comparative Analysis 

1. Training Performance 

The surrogate model achieved strong baseline accuracy (≈ 93 %), confirming it could learn stable identity-related features before attacks. 

The training curve (Figure 5.1) shows rapid convergence within 3 epochs and minimal overfitting. 

2. Adversarial Generation and Restoration Outcomes 
- **BIM attacks:** Introduced visible pixel noise; all 19 962 test images successfully perturbed. 
- **FLM attacks:** Produced plausible yet semantically altered faces (landmark warps). 
- **Restoration:** Processed all images in ≈ 50 seconds (> 390 images/s), restoring image smoothness and realism. 

![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.009.jpeg)

***Figure 6.1: Visual Triplets (Original–Adversarial–Restored)*** 

3. WLMP Classification Results 

**Classifier  Accuracy Before Restoration (%) Accuracy After Restoration (%) Change (Δ)** Linear SVM  77.5  69.7  – 7.8 RBF-SVM  81.3  72.7  – 8.6 Random Forest  97.2  96.5  – 0.7 

k-NN  97.3  96.8  – 0.5 

![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.010.jpeg)

***Figure 6.2: WLMP Classifier Accuracy Before vs After Restoration*** 

![](Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.011.png)

***Figure 6.3: Change in Accuracy After Restoration*** 

**Interpretation:** 

Across classifiers, restoration reduced adversarial detectability by roughly **10–13 %** on average, implying that bilateral smoothing removed discriminative perturbation patterns. 

This confirms that the restoration step successfully “pushes” adversarial samples closer to the clean image manifold. 

4. Comparison with the Base Paper 

**Base Paper  After Restoration  Trend in This Work** 

**Attack Type  Direction Agreement** 

**Accuracy (Before)  (Paper)  (WLMP Δ %)** 

P-FGSM / BIM 

98\.75  **99.00** 

(pixel) 

StyleGAN 

77\.94  **85.25** 

(generative) 

FLM (geometric)  65.52  **69.50** 

– 14 

Not tested 

Slight reduction in detectability 

Same direction (improvement) 

— 

Partial agreement 

While the paper measures **recognition accuracy restoration**, this implementation quantifies **adversarial detectability reduction**. Both the techniques indicate that restoration neutralizes attack artifacts. 

The observed trends match directionally: restoration improves robustness (or hides adversarial traces) in all cases. 

5. Sources of Discrepancy 

**Factor  Explanation  Impact on Results** 

The full RobustFace BL + SR network (blind deblurring + 

**Simplified**  Reduced restoration precision; 

super-resolution) was replaced by a simple bilateral 

**Restoration**  smaller accuracy gains. 

filter. 

CelebA images were converted to L-channel; color cues Texture descriptors (WLMP) **Grayscale Input** 

were lost.  became less discriminative. 

The paper measured recognition accuracy; this work  Quantitative values not directly **Different Metric** 

measured WLMP-based detectability.  comparable. 

**No Adaptive**  Adaptive re-optimization through the defense not  Possibly overestimates **Attack Testing**  implemented.  robustness margins. 

SmallFaceNet is simpler than state-of-the-art  Limits baseline robustness and **Lightweight CNN** 

recognition models.  restoration effects. 

6. Key Findings 
1. **Qualitative Consistency:** Both the paper and this implementation show restoration reduces adversarial distortions. 
1. **Quantitative Gap:** Smaller accuracy shifts stem from the simplified, non-learned restoration. 
3. **Efficiency:** The bilateral filter achieved ~390 images/s processing speed which is ideal for edge or client-side privacy defenses. 
3. **WLMP Insight:** Post-restoration WLMP histograms became smoother, indicating reduced adversarial texture contrast. 
7. Overall Interpretation 

The reproduction confirms the **core principle of RobustFace**: adversarial robustness can be enhanced through image restoration that reprojects faces toward the natural manifold. Even a minimal bilateral filtering step demonstrated measurable mitigation, validating the conceptual model proposed by Sadu et al. (2024). 

[ref1]: Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.005.jpeg
[ref2]: Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.006.jpeg
[ref3]: Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.007.jpeg
[ref4]: Aspose.Words.26092f26-0017-4609-b30c-95d5828da621.008.jpeg
